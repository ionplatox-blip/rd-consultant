#!/usr/bin/env python3
"""
RAG System Benchmark Test
==========================
Tests the AI R&D Consultant with 20 diverse questions to verify:
1. System is using the knowledge base (not hallucinating)
2. Retrieval quality (relevant chunks are found)
3. Answer accuracy and completeness

Usage:
    python3 scripts/benchmark_rag.py --endpoint local
    python3 scripts/benchmark_rag.py --endpoint production
"""

import requests
import json
import time
from typing import List, Dict
import argparse

# Test questions covering different aspects of R&D knowledge base
BENCHMARK_QUESTIONS = [
    # Tax benefits and –ª—å–≥–æ—Ç—ã
    {
        "id": 1,
        "question": "–ö–∞–∫–∏–µ –Ω–∞–ª–æ–≥–æ–≤—ã–µ –ª—å–≥–æ—Ç—ã —Å—É—â–µ—Å—Ç–≤—É—é—Ç –¥–ª—è –ù–ò–û–ö–† –≤ –†–æ—Å—Å–∏–∏?",
        "category": "tax_benefits",
        "expected_keywords": ["–Ω–∞–ª–æ–≥", "–ª—å–≥–æ—Ç", "–≤—ã—á–µ—Ç", "–ù–ò–û–ö–†"]
    },
    {
        "id": 2,
        "question": "–ö–∞–∫ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –Ω–∞–ª–æ–≥–æ–≤—ã–π –≤—ã—á–µ—Ç –Ω–∞ –ù–ò–û–ö–†?",
        "category": "tax_calculation",
        "expected_keywords": ["—Ä–∞—Å—á–µ—Ç", "–≤—ã—á–µ—Ç", "–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç"]
    },
    {
        "id": 3,
        "question": "–ö–∞–∫–∏–µ —Ä–∞—Å—Ö–æ–¥—ã –º–æ–∂–Ω–æ –≤–∫–ª—é—á–∏—Ç—å –≤ –Ω–∞–ª–æ–≥–æ–≤—ã–π –≤—ã—á–µ—Ç –ø–æ –ù–ò–û–ö–†?",
        "category": "tax_expenses",
        "expected_keywords": ["—Ä–∞—Å—Ö–æ–¥", "–∑–∞—Ç—Ä–∞—Ç", "–≤–∫–ª—é—á"]
    },
    
    # Documentation and –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ
    {
        "id": 4,
        "question": "–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã –¥–ª—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –ù–ò–û–ö–†?",
        "category": "documentation",
        "expected_keywords": ["–¥–æ–∫—É–º–µ–Ω—Ç", "–ø–æ–¥—Ç–≤–µ—Ä–∂–¥", "–æ—Ñ–æ—Ä–º"]
    },
    {
        "id": 5,
        "question": "–ß—Ç–æ —Ç–∞–∫–æ–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ –Ω–∞ –ù–ò–û–ö–†?",
        "category": "technical_spec",
        "expected_keywords": ["—Ç–µ—Ö–Ω–∏—á–µ—Å–∫", "–∑–∞–¥–∞–Ω", "–¢–ó"]
    },
    {
        "id": 6,
        "question": "–ö–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ñ–æ—Ä–º–∏—Ç—å –æ—Ç—á–µ—Ç –æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –ù–ò–û–ö–†?",
        "category": "reporting",
        "expected_keywords": ["–æ—Ç—á–µ—Ç", "–æ—Ñ–æ—Ä–º", "–≤—ã–ø–æ–ª–Ω–µ–Ω"]
    },
    
    # Accounting and —É—á–µ—Ç
    {
        "id": 7,
        "question": "–ö–∞–∫ —É—á–∏—Ç—ã–≤–∞—é—Ç—Å—è —Ä–∞—Å—Ö–æ–¥—ã –Ω–∞ –ù–ò–û–ö–† –≤ –±—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–æ–º —É—á–µ—Ç–µ?",
        "category": "accounting",
        "expected_keywords": ["—É—á–µ—Ç", "—Ä–∞—Å—Ö–æ–¥", "–±—É—Ö–≥–∞–ª—Ç–µ—Ä"]
    },
    {
        "id": 8,
        "question": "–ß—Ç–æ —Ç–∞–∫–æ–µ –ù–ú–ê –∏ –∫–∞–∫ –æ–Ω–∏ —Å–≤—è–∑–∞–Ω—ã —Å –ù–ò–û–ö–†?",
        "category": "intangible_assets",
        "expected_keywords": ["–ù–ú–ê", "–Ω–µ–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω", "–∞–∫—Ç–∏–≤"]
    },
    {
        "id": 9,
        "question": "–ö–∞–∫–∏–µ –µ—Å—Ç—å –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∞–º–æ—Ä—Ç–∏–∑–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ù–ò–û–ö–†?",
        "category": "amortization",
        "expected_keywords": ["–∞–º–æ—Ä—Ç–∏–∑–∞—Ü", "—Å–ø–∏—Å–∞–Ω", "—Å—Ä–æ–∫"]
    },
    
    # Grants and —Å—É–±—Å–∏–¥–∏–∏
    {
        "id": 10,
        "question": "–ö–∞–∫–∏–µ –≥—Ä–∞–Ω—Ç—ã –¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏—è –ù–ò–û–ö–†?",
        "category": "grants",
        "expected_keywords": ["–≥—Ä–∞–Ω—Ç", "—Å—É–±—Å–∏–¥–∏", "—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤"]
    },
    {
        "id": 11,
        "question": "–ß—Ç–æ —Ç–∞–∫–æ–µ —Ñ–æ–Ω–¥ –ú–ò–ö –∏ –∫–∞–∫ –ø–æ–ª—É—á–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫—É?",
        "category": "mik_fund",
        "expected_keywords": ["–ú–ò–ö", "—Ñ–æ–Ω–¥", "–ø–æ–¥–¥–µ—Ä–∂–∫"]
    },
    
    # Patents and IP
    {
        "id": 12,
        "question": "–ö–∞–∫ –æ—Ñ–æ—Ä–º–∏—Ç—å –ø–∞—Ç–µ–Ω—Ç –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ù–ò–û–ö–†?",
        "category": "patents",
        "expected_keywords": ["–ø–∞—Ç–µ–Ω—Ç", "–æ—Ñ–æ—Ä–º", "—Ä–µ–∑—É–ª—å—Ç–∞—Ç"]
    },
    {
        "id": 13,
        "question": "–ß—Ç–æ —Ç–∞–∫–æ–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –ù–ò–û–ö–†?",
        "category": "ip",
        "expected_keywords": ["–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω", "—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç", "–ò–°"]
    },
    
    # Criteria and –∫—Ä–∏—Ç–µ—Ä–∏–∏
    {
        "id": 14,
        "question": "–ö–∞–∫–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ä–∞–±–æ—Ç–∞ –ù–ò–û–ö–†?",
        "category": "criteria",
        "expected_keywords": ["–∫—Ä–∏—Ç–µ—Ä–∏", "–æ–ø—Ä–µ–¥–µ–ª", "–ø—Ä–∏–∑–Ω–∞–∫"]
    },
    {
        "id": 15,
        "question": "–ß–µ–º –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –ù–ò–û–ö–† –æ—Ç –æ–±—ã—á–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏?",
        "category": "distinction",
        "expected_keywords": ["–æ—Ç–ª–∏—á", "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫", "–Ω–∞—É—á–Ω"]
    },
    
    # Risks and —Ä–∏—Å–∫–∏
    {
        "id": 16,
        "question": "–ö–∞–∫–∏–µ —Ä–∏—Å–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –ø—Ä–∏ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–∏ –ù–ò–û–ö–†?",
        "category": "risks",
        "expected_keywords": ["—Ä–∏—Å–∫", "–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç", "–Ω–µ—É–¥–∞—á"]
    },
    
    # Stages and —ç—Ç–∞–ø—ã
    {
        "id": 17,
        "question": "–ö–∞–∫–∏–µ —ç—Ç–∞–ø—ã –≤–∫–ª—é—á–∞–µ—Ç –ø—Ä–æ–µ–∫—Ç –ù–ò–û–ö–†?",
        "category": "stages",
        "expected_keywords": ["—ç—Ç–∞–ø", "—Å—Ç–∞–¥–∏", "—Ñ–∞–∑"]
    },
    
    # Personnel and –ø–µ—Ä—Å–æ–Ω–∞–ª
    {
        "id": 18,
        "question": "–ö–∞–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø–µ—Ä—Å–æ–Ω–∞–ª—É –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ù–ò–û–ö–†?",
        "category": "personnel",
        "expected_keywords": ["–ø–µ—Ä—Å–æ–Ω–∞–ª", "—Å–æ—Ç—Ä—É–¥–Ω–∏–∫", "–∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü"]
    },
    
    # Edge cases - should return "no info"
    {
        "id": 19,
        "question": "–ö–∞–∫–∞—è –ø–æ–≥–æ–¥–∞ –±—É–¥–µ—Ç –∑–∞–≤—Ç—Ä–∞?",
        "category": "irrelevant",
        "expected_keywords": [],
        "should_have_no_answer": True
    },
    {
        "id": 20,
        "question": "–ö–∞–∫ –ø—Ä–∏–≥–æ—Ç–æ–≤–∏—Ç—å –±–æ—Ä—â?",
        "category": "irrelevant",
        "expected_keywords": [],
        "should_have_no_answer": True
    }
]

def test_endpoint(endpoint_url: str, question_data: Dict) -> Dict:
    """Test a single question against the RAG endpoint."""
    start_time = time.time()
    
    try:
        response = requests.post(
            f"{endpoint_url}/api/chat",
            json={"message": question_data["question"]},
            timeout=30
        )
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            data = response.json()
            return {
                "success": True,
                "answer": data.get("answer", ""),
                "sources": data.get("sources", []),
                "response_time": elapsed,
                "error": None
            }
        else:
            return {
                "success": False,
                "answer": "",
                "sources": [],
                "response_time": elapsed,
                "error": f"HTTP {response.status_code}"
            }
    except Exception as e:
        elapsed = time.time() - start_time
        return {
            "success": False,
            "answer": "",
            "sources": [],
            "response_time": elapsed,
            "error": str(e)
        }

def evaluate_response(question_data: Dict, result: Dict) -> Dict:
    """Evaluate the quality of a response."""
    evaluation = {
        "has_answer": len(result["answer"]) > 0,
        "has_sources": len(result["sources"]) > 0,
        "answer_length": len(result["answer"]),
        "source_count": len(result["sources"]),
        "keywords_found": 0,
        "quality_score": 0
    }
    
    # Check if this is an irrelevant question
    if question_data.get("should_have_no_answer", False):
        # For irrelevant questions, good response is "no info" type answer
        no_info_phrases = ["–Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "–Ω–µ –Ω–∞—à—ë–ª", "–Ω–µ –º–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å", "–∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç"]
        has_no_info_response = any(phrase in result["answer"].lower() for phrase in no_info_phrases)
        evaluation["quality_score"] = 100 if has_no_info_response else 0
        evaluation["is_honest_no_answer"] = has_no_info_response
        return evaluation
    
    # Check for expected keywords
    answer_lower = result["answer"].lower()
    for keyword in question_data.get("expected_keywords", []):
        if keyword.lower() in answer_lower:
            evaluation["keywords_found"] += 1
    
    # Calculate quality score (0-100)
    score = 0
    
    # Has answer (30 points)
    if evaluation["has_answer"]:
        score += 30
    
    # Has sources (20 points)
    if evaluation["has_sources"]:
        score += 20
    
    # Answer length (20 points)
    if evaluation["answer_length"] > 200:
        score += 20
    elif evaluation["answer_length"] > 100:
        score += 10
    
    # Keywords found (30 points)
    if len(question_data.get("expected_keywords", [])) > 0:
        keyword_ratio = evaluation["keywords_found"] / len(question_data["expected_keywords"])
        score += int(keyword_ratio * 30)
    
    evaluation["quality_score"] = score
    return evaluation

def run_benchmark(endpoint_url: str):
    """Run the full benchmark test suite."""
    print(f"\n{'='*80}")
    print(f"RAG SYSTEM BENCHMARK TEST")
    print(f"Endpoint: {endpoint_url}")
    print(f"Questions: {len(BENCHMARK_QUESTIONS)}")
    print(f"{'='*80}\n")
    
    results = []
    total_time = 0
    
    for i, question_data in enumerate(BENCHMARK_QUESTIONS, 1):
        print(f"[{i}/{len(BENCHMARK_QUESTIONS)}] Testing: {question_data['question'][:60]}...")
        
        result = test_endpoint(endpoint_url, question_data)
        evaluation = evaluate_response(question_data, result)
        
        results.append({
            "question": question_data,
            "result": result,
            "evaluation": evaluation
        })
        
        total_time += result["response_time"]
        
        # Print quick result
        if result["success"]:
            score = evaluation["quality_score"]
            emoji = "‚úÖ" if score >= 70 else "‚ö†Ô∏è" if score >= 40 else "‚ùå"
            print(f"  {emoji} Score: {score}/100 | Sources: {evaluation['source_count']} | Time: {result['response_time']:.2f}s\n")
        else:
            print(f"  ‚ùå Error: {result['error']}\n")
        
        # Small delay to avoid rate limiting
        time.sleep(0.5)
    
    # Print summary
    print(f"\n{'='*80}")
    print(f"BENCHMARK RESULTS SUMMARY")
    print(f"{'='*80}\n")
    
    successful = sum(1 for r in results if r["result"]["success"])
    avg_score = sum(r["evaluation"]["quality_score"] for r in results) / len(results)
    avg_time = total_time / len(results)
    
    print(f"‚úÖ Successful requests: {successful}/{len(BENCHMARK_QUESTIONS)}")
    print(f"üìä Average quality score: {avg_score:.1f}/100")
    print(f"‚è±Ô∏è  Average response time: {avg_time:.2f}s")
    print(f"üìö Questions with sources: {sum(1 for r in results if r['evaluation']['has_sources'])}/{len(BENCHMARK_QUESTIONS)}")
    
    # Category breakdown
    print(f"\n{'='*80}")
    print(f"CATEGORY BREAKDOWN")
    print(f"{'='*80}\n")
    
    categories = {}
    for r in results:
        cat = r["question"]["category"]
        if cat not in categories:
            categories[cat] = []
        categories[cat].append(r["evaluation"]["quality_score"])
    
    for cat, scores in sorted(categories.items()):
        avg_cat_score = sum(scores) / len(scores)
        emoji = "‚úÖ" if avg_cat_score >= 70 else "‚ö†Ô∏è" if avg_cat_score >= 40 else "‚ùå"
        print(f"{emoji} {cat:20s}: {avg_cat_score:.1f}/100 ({len(scores)} questions)")
    
    # Detailed results
    print(f"\n{'='*80}")
    print(f"DETAILED RESULTS")
    print(f"{'='*80}\n")
    
    for r in results:
        q = r["question"]
        ev = r["evaluation"]
        res = r["result"]
        
        emoji = "‚úÖ" if ev["quality_score"] >= 70 else "‚ö†Ô∏è" if ev["quality_score"] >= 40 else "‚ùå"
        print(f"{emoji} Q{q['id']}: {q['question']}")
        print(f"   Category: {q['category']}")
        print(f"   Score: {ev['quality_score']}/100")
        print(f"   Answer length: {ev['answer_length']} chars")
        print(f"   Sources: {ev['source_count']}")
        print(f"   Keywords found: {ev['keywords_found']}/{len(q.get('expected_keywords', []))}")
        if res["success"]:
            print(f"   Answer preview: {res['answer'][:150]}...")
        else:
            print(f"   Error: {res['error']}")
        print()
    
    # Save results to JSON
    output_file = f"benchmark_results_{int(time.time())}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"üìÑ Full results saved to: {output_file}")
    
    return results, avg_score

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Benchmark RAG system")
    parser.add_argument(
        "--endpoint",
        choices=["local", "production"],
        default="local",
        help="Which endpoint to test"
    )
    
    args = parser.parse_args()
    
    if args.endpoint == "local":
        endpoint_url = "http://localhost:3005"
    else:
        endpoint_url = "https://rd-consultant-ionplato.onrender.com"
    
    results, avg_score = run_benchmark(endpoint_url)
    
    # Exit code based on quality
    if avg_score >= 70:
        print("\n‚úÖ Benchmark PASSED (score >= 70)")
        exit(0)
    elif avg_score >= 40:
        print("\n‚ö†Ô∏è  Benchmark MARGINAL (40 <= score < 70)")
        exit(1)
    else:
        print("\n‚ùå Benchmark FAILED (score < 40)")
        exit(2)
